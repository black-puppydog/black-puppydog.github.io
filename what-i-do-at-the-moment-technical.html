<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Daan Wynen" />
    <meta name="robots" content="index, follow"/>

    <meta property="og:title" content="What I do at the moment (the technical version)"/>
    <meta property="og:url" content="./what-i-do-at-the-moment-technical.html"/>
    <meta property="og:site_name" content="Some Personal Notes"/>
    <meta property="og:type" content="article"/>

    <link rel="canonical" href="./what-i-do-at-the-moment-technical.html" />

    <title>What I do at the moment (the technical version) | Some Personal Notes</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="./theme/css/font-awesome.css" />
    <link rel="stylesheet" type="text/css" href="./theme/css/bootstrap-combined.min.css" />

    <link rel="stylesheet" type="text/css" href="./theme/css/main.css" />

    <script type="text/javascript">var switchTo5x=true;</script>
    <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
    <script type="text/javascript">
        stLight.options({
            publisher: "",
            doNotHash: false,
            doNotCopy: false,
            hashAddressBar: false
        });
    </script>
</head>

<body id="index">
    <div class="row-fluid">
        <div class="span10 offset1">
            <header id="banner" >
                <h1>
                    <a href="./">Some Personal Notes </a>
                </h1>
                <nav class="navbar">
                    <div class="navbar-inner">
                        <ul class="nav">
                            <li class="active"><a href="./category/programming.html">Programming</a></li>
                            <li ><a href="./category/war-on-democracy.html">War on Democracy</a></li>
                            <li ><a href="./category/web.html">Web</a></li>
                        </ul>

                    </div>
                </nav>
            </header><!-- /#banner -->
        </div>
    </div>

    <div class="row-fluid">
        <div class="span10 offset1">
            <div class="row-fluid">
<div class="span10 offset1">
  <section>
    <article>
      <header>
        <h1 class="entry-title">
          <a href="./what-i-do-at-the-moment-technical.html" rel="bookmark"
             title="Permalink to What I do at the moment (the technical version)">What I do at the moment (the technical version)</a></h1>
      </header>
      <div class="entry-content">
<footer class="post-info">
    <address class="vcard author">
        by <a class="url fn" href="./author/daan-wynen.html">Daan Wynen</a>
    </address>

    in <a href="./category/programming.html">Programming</a>

    on 2015-03-03

        |
        tags:         <a href="./tag/thesis.html">Thesis</a>
        <a href="./tag/machine-learning.html">Machine Learning</a>
        <a href="./tag/ckn.html">CKN</a>
        <a href="./tag/programming.html">Programming</a>



    
</footer><!-- /.post-info -->

        <p>Some people have asked me what I do for my thesis, and I always struggle to explain it in a concise way, so I thought I might just as well just write it down.
This post is for people familiar with Machine Learning methods, another one for people who don't fall into this category is on its way.
So, here goes...</p>
<h2>The Nutshell Version</h2>
<p>My thesis is about <a href="http://arxiv.org/abs/1406.3332">Convolutional Kernel Networks</a>.
They solve the <span class="math">\(N^2\)</span> problem of kernel methods by using a neural network to compute N kernel descriptors instead of the full Gram matrix, followed by a linear SVM.
This will hopefully allow very complex (read: expensive to compute) kernel functions to be designed and computed on all kinds of data, and on large datasets.
I work on the first generalization from images to videos, in particular for action recognition.</p>
<h2>The Technical Version</h2>
<p>CKNs were develoed by my supervisor <a href="http://lear.inrialpes.fr/people/mairal/index_eng.php">Julien Mairal</a> and other people from team <a href="http://lear.inrialpes.fr/">LEAR</a> over the last year and a bit.
They combine <em>neural networks</em> and <em>kernel methods</em></p>
<h3>Reminder: Kernel Methods</h3>
<p>Kernel methods make use of a very handy property of data: the higher the dimensionality of the input data, the higher the probability that they can be separated by a hyperplane.
For a two-class problem with classes <span class="math">\(1\)</span> and <span class="math">\(-1\)</span> we  <span class="math">\( w^\intercal x + b\)</span>, where <span class="math">\(w,x \in \mathbb{R}^p\)</span> </p>
<p>To take advantage of that, the input data can be <em>nonlinearly</em> transformed to a higher-dimensional space, where (hopefully) they will be easier to separate.
To find such a hyperplane one can use a Support Vector Machine.
And with a bit of extra work, one can even avoid actually computing the transformed data. 
This is where the "kernel" comes into the method: instead of computing the tranforms of two inputs and then the scalar product, a kernel function directly computes the scalar product.
This way even infinite-dimensional tranformations are possible, since they are never explicitely computed.</p>
<p>These methods were hugely popular for a while, but suffer from some serious drawbacks.
Specifically their memory requirement when storing the full Gram matrix (the matrix holding the pair-wise kernel values) for <span class="math">\(N\)</span> examples is <span class="math">\(N^2\)</span>.
For 1 million examples, this would mean storing 8 TB of data.
And who really wants to settle for a mere <em>million</em> of training examples? :P</p>
<p>CKNs address this storage problem by using neural networks to produce kernel <em>descriptors</em>.
When the inputs are fed to the network they yield one descriptor each.
The trick is that the network is trained so that the the inner product of two descriptors approximates their kernel.
Basically, it replaces a kernel operating in a very high-dimensional (possibly infinite-dimensional) space with one operating on a small or small-ish space.
The resulting kernel descriptors can then be used to train a linear SVM, which has the advantage of not necessarily needing the full Gram matrix.
So we go from <span class="math">\(O(n^2)\)</span> down to <span class="math">\(O(n)\)</span>.
Now we "just" have to take care that the constant factors don't mess this up again.</p>
<p>My task at the moment is to take this method, and see how it might be transferred to Video data.
For me personally that means learning loads of stuff about different aspects of this problem:</p>
<p><strong>Optimization:</strong> Sure, I had seen some Linear Programming, Cutting plane and and Branch-and-Bound algorithms, and perceptron learning and such was part of the Machine Learning classes. 
But getting hands-on experience with different optimization algorithms is something else, and I realize there is a really interesting mix to be found between hard bounds and guarantees on the one side and intuition for how the different algorithms behave on the other side.</p>
<p><strong>Video data:</strong> 
in retrospect it is quite obvious what happens when you go from images to videos. 
Everything just becomes bigger. 
But that doesn't mean I saw that coming... 
One after the other, the objects I was handling in my code refused to fit into main memory. 
So that means I am picking up a lot of new skills in building a proper pipeline that can handle this.</p>
<h2>Why does that help? And what does it help with?</h2>
<p>Take <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> as an example: <span class="math">\(50,000\)</span> training examples, that already gives us <span class="math">\(50,000^2 = 2,500,000,000\)</span> numbers to store. 
At 8 bytes per <code>double</code>, that amounts to roughly <strong>20 GB</strong>. 
And MNIST is a "small" dataset.
Try this for the <a href="http://cs.stanford.edu/people/karpathy/deepvideo/">Sports-1M</a> dataset released by Google last year: 
$N = 1,000,000 \Rightarrow N^2 * 8 = $ <strong>8 TB</strong>!
And obviously, this trend will hardly stop.</p>
<h2>The Non-Technical Version</h2>
<p>Let me start with some really basic introduction and a bit of not-so-old history.</p>
<h3>Detection, Recognition, Classification</h3>
<p>At the end of the 90s <a href="http://yann.lecun.com/exdb/mnist/">digit recognition</a> (very helpful for automatically sorting mail for example) was becoing an "easy" problem and new challenges and solutions have been showing up ever since.
At the moment, computers can beat humans in several tasks, and are very close to doing so in others.
In the last years Deep Neural Networks have taken over the field, and for images in particular a specialization called Convolutional Neural Networks are <a href="https://www.facebook.com/yann.lecun/posts/10152208001087143">very popular</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }, linebreaks: { automatic: false, width: 'container' }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

      </div><!-- /.entry-content -->

    </article>
  </section>
</div>
            </div>
        </div>
    </div>

    <footer id="site-footer">
        <div class="row-fluid">
            <div class="span10 offset1">
                <address>
                    <p>
                        This blog is proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                    </p>
                    <p>
                        <a href="http://github.com/jsliang/pelican-fresh/">Fresh</a> is a responsive theme designed by <a href="http://jsliang.com/">jsliang</a> and <a href="https://github.com/jsliang/pelican-fresh/graphs/contributors">contributors</a>.
                        Special thanks to <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a> and <a href="http://getbootstrap.com/">Twitter Bootstrap</a>.
                    </p>
                </address>
            </div>
        </div>
    </footer>

    <script src="//code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>
</body>
</html>